---
title: "6-Clustering"
output:
  html_document:
    df_print: paged
    toc: true
  html_notebook:
    highlight: textmate
    theme: cerulean
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
dir.create("images")
```

***

# K-Means Example: Wine Sales

We illustrate the use of k-means through the "wine" example. First, install the required packages
```{r}
library(tidyverse)  # data manipulation
# install.packages("factoextra") # if necessary
library(factoextra) # clustering algorithms & visualization
library(ISLR)
set.seed(123)

library(readr)
wine.df <- read_csv("~/Downloads/wine-clustering.csv")
head(wine.df)


```

We will cluster wine, but, first, we must scale the data.
```{r}
# Scaling the data frame (z-score) 
df <- scale(wine.df)
distance <- get_dist(df)
fviz_dist(distance)
```

The above graph shows the distance between wines.We will choose an initial value of k = 4.
```{r}
k4 <- kmeans(df, centers = 4, nstart = 25) # k = 4, number of restarts = 25

# Visualize the output

k4$centers # output the centers

k4$size # Number  in each cluster

k4$cluster[120] # Identify the cluster of the 120th observation as an example

fviz_cluster(k4, data = df) # Visualize the output
```



***

# Other Distances

Let us now rerun the example using other distances
```{r}
#install.packages("flexclust")
library(flexclust)
set.seed(123)
#kmeans clustering, using manhattan distance
k4 = kcca(df, k=4, kccaFamily("kmedians"))
k4
```

Let us now apply the predict function
```{r}
#Apply the predict() function
clusters_index <- predict(k4)
dist(k4@centers)
image(k4)
points(df, col=clusters_index, pch=19, cex=0.3)
```

***

# Determining k

Let us use an "elbow chart" to determine k
```{r}
library(tidyverse)  # data manipulation
library(factoextra) # clustering & visualization
library(ISLR)
set.seed(123)


fviz_nbclust(df, kmeans, method = "wss")
```

The chart shows that the elbow point 3 provides the best value for k. While WSS will continue to drop for larger values of k, we have to make the tradeoff between overfitting, i.e., a model fitting both noise and signal, to a model having bias. Here, the elbow point provides that compromise where WSS, while still decreasing beyond k = 3, decreases at a much smaller rate. In other words, adding more clusters beyond 4 brings less improvement to cluster homogeneity.

***

## Silhouette Method

Let us now apply the Silhouette Method to determine the number of clusters
```{r}
fviz_nbclust(df, kmeans, method = "silhouette")
```

Again, we see that 3 is the ideal number of clusters. Here we look for large values for the Silhouette Width (Y Axis)


# dbscan Example

```{r}
# install.packages("dbscan") # install if necessary
library("dbscan") 
#install.packages("fpc")
library("fpc")
library('factoextra')
#moons dataset Contains 100 2-D points, half of which are contained in two # moons or “blobs” (25 points each blob), and the other half in asymmetric # facing crescent shapes.
  
plot(wine.df[,1:2], pch=20) # plot original data points

```

The data shows both nonlinearity and different densities. 

***

Let us apply dbscan to this example
```{r}
db <- dbscan::dbscan(wine.df, eps = 25, minPts = 5) #perform clustering

print(db) #print cluster details

plot(db, wine.df[,1:2], main = "DBSCAN", frame = FALSE) #plot cluster details
```

***

# fpc Package

Let us now apply the fpc package to an example
```{r}
# install_packages("fpc")
library('factoextra')
library('fpc')


set.seed(123)
db <- fpc::dbscan(wine.df, eps = 25, MinPts = 5) # DBSCAN using fpc package

print(db) # show clusters' details

```

The results show that there are a total of 31 border points. The values under the row Seed indicate the core points. 

```{r}
# Plot DBSCAN results
plot(db, wine.df[,1:2], main = "DBSCAN", frame = FALSE)
```

The colored triangles indicate core points, while the colored circles border points. The black circles indicates noise (outliers).

Here is an alternative way to depict the same plot
```{r}
fviz_cluster(db, wine.df[,1:2],   stand = FALSE, frame = FALSE, geom = "point") # Alternative way to depict plot
```

***

## Optimal eps value

dbscan is sensitive to changes in eps values, as the below example illustrates.
```{r}
db <- fpc::dbscan(wine.df, eps = 25, MinPts = 5)
print(db)
plot(db, wine.df[,1:2], main = "DBSCAN", frame = FALSE)
```

As you can clearly see, changing the neighborhood changes the categorization of the points. Many of the noise points now belong to a cluster, and there are fewer border points.

***

To determine the optimal eps, we compute the the k-nearest neighbor distances in a matrix of points.The idea is to calculate, the average of the distances of every point to its k nearest neighbors. The value of k will be specified by the user and corresponds to MinPts. Next, these k-distances are plotted in an ascending order. The aim is to determine the “knee”, which corresponds to the optimal eps parameter.
```{r}
dbscan::kNNdistplot(wine.df,k=3)
abline(h=30,lty=2)
```

